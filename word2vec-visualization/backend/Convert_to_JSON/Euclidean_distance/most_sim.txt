
    def most_similar_euclidean(self, positive=[], negative=[], topn=10):
        #Author: Phi Van thuy
        #For Euclidean distance
        self.init_sims()

        if isinstance(positive, string_types) and not negative:
            # allow calls like most_similar('dog'), as a shorthand for most_similar(['dog'])
            positive = [positive]

        # add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words
        positive = [(word, 1.0) if isinstance(word, string_types + (ndarray,))
                                else word for word in positive]
        negative = [(word, -1.0) if isinstance(word, string_types + (ndarray,))
                                 else word for word in negative]

        # compute the weighted average of all words
        all_words, mean = set(), []
        for word, weight in positive + negative:
            if isinstance(word, ndarray):
                mean.append(weight * word)
            elif word in self.vocab:
                mean.append(weight * self.syn0norm[self.vocab[word].index])
                all_words.add(self.vocab[word].index)
            else:
                raise KeyError("word '%s' not in vocabulary" % word)
        if not mean:
            raise ValueError("cannot compute similarity with no input")
        mean = matutils.unitvec(array(mean).mean(axis=0)).astype(REAL)

        #dists = dot(self.syn0norm, mean)

        mean = np.reshape(mean, (1, 200))

        dists = distance.cdist(self.syn0norm, mean, 'euclidean')
        dists = np.reshape(dists, (len(self.vocab), ))
        #print dists.shape

        if not topn:
            return dists
        best = argsort(dists)[::-1][:topn + len(all_words)]
        #print best.shape
        # ignore (don't return) words from the input
        result = [(self.index2word[sim], float(dists[sim])) for sim in best if sim not in all_words]
        return result[:topn]
